{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* torch: Core PyTorch library, which provides tensor operations.\n",
    "* torch.nn: Contains neural network layers like Conv2d, Linear, etc.\n",
    "* torch.nn.functional: Provides functions for operations like activation functions (ReLU), pooling, etc.\n",
    "* torch.optim: Contains optimization algorithms such as SGD (Stochastic Gradient Descent).\n",
    "* torchvision: Useful for image datasets and transformations. Here, we use it to load the MNIST dataset and apply transformations like normalization.\n",
    "\n",
    "\n",
    "1. SimpleCNN(nn.Module): We define a class SimpleCNN, which inherits from nn.Module (the base class for all neural networks in PyTorch). This will allow us to build a custom architecture.\n",
    "2. self.conv1 = nn.Conv2d(1, 32, kernel_size=3):\n",
    "\n",
    "    This is a convolutional layer that takes a grayscale image (with 1 channel), applies 32 filters, each of size 3x3.\n",
    "    stride=1: The filter moves 1 pixel at a time.\n",
    "    padding=1: Adds a 1-pixel border around the image to ensure the output has the same spatial dimensions as the input.\n",
    "\n",
    "3. self.conv2 = nn.Conv2d(32, 64, kernel_size=3):\n",
    "\n",
    "    A second convolutional layer that takes the 32 feature maps from the first layer and applies 64 filters of size 3x3.\n",
    "\n",
    "4. self.pool = nn.MaxPool2d(2, 2):\n",
    "\n",
    "    A max-pooling layer with a 2x2 window and stride 2. This reduces the spatial dimensions of the feature map by a factor of 2, which helps in downsampling the data and reducing computation.\n",
    "\n",
    "5. self.fc1 = nn.Linear(64 * 7 * 7, 128):\n",
    "\n",
    "    A fully connected layer that takes the flattened feature maps from the previous layer. Since the feature maps from the last convolution have a size of 64x7x7, the input to the fully connected layer is 6477.\n",
    "\n",
    "6. self.fc2 = nn.Linear(128, 10):\n",
    "\n",
    "    Another fully connected layer that maps the 128 features from the previous layer to 10 output units (corresponding to 10 classes, e.g., digits 0â€“9 for MNIST)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = self.pool(F.relu(self.conv1(x))):\n",
    "\n",
    "    First, we apply the first convolutional layer conv1, followed by the ReLU activation function (F.relu), and then we downsample using max-pooling (self.pool).\n",
    "\n",
    "x = x.view(-1, 64 * 7 * 7):\n",
    "\n",
    "    After the convolutional and pooling layers, the data is still in 2D format. This line flattens the 2D feature maps into a 1D vector so they can be passed to the fully connected layers.\n",
    "\n",
    "x = F.relu(self.fc1(x)):\n",
    "\n",
    "    The flattened vector is passed through the first fully connected layer fc1, followed by the ReLU activation.\n",
    "\n",
    "x = self.fc2(x):\n",
    "\n",
    "    The result from fc1 is passed to the final fully connected layer fc2, which outputs 10 class scores (logits), one for each digit (0-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size: The number of training samples to work through before updating the model's weights.\n",
    "learning_rate: Controls how much to adjust the model's weights during each optimization step.\n",
    "epochs: The number of complete passes through the training dataset.\n",
    "\n",
    "transforms.Compose: Applies transformations to the images. Here:\n",
    "\n",
    "    ToTensor(): Converts the images to tensors.\n",
    "    Normalize((0.5,), (0.5,)): Normalizes the images by subtracting the mean (0.5) and dividing by the standard deviation (0.5), so pixel values fall within [-1, 1].\n",
    "\n",
    "datasets.MNIST: Loads the MNIST dataset. It downloads the dataset if not already present.\n",
    "\n",
    "torch.utils.data.DataLoader: Loads the data in batches (size of batch_size), and the shuffle=True ensures the data is randomly shuffled at each epoch.\n",
    "\n",
    "\n",
    "model = SimpleCNN(): Creates an instance of our SimpleCNN class.\n",
    "criterion = nn.CrossEntropyLoss(): We use the cross-entropy loss function, suitable for multi-class classification problems.\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate): Stochastic Gradient Descent is used to update the weights of the model parameters with the learning rate of 0.01.\n",
    "\n",
    "for epoch in range(epochs): Loops over the dataset multiple times.\n",
    "optimizer.zero_grad(): Clears the gradients from the previous iteration.\n",
    "outputs = model(images): Performs a forward pass through the network to get predictions for the current batch of images.\n",
    "loss = criterion(outputs, labels): Calculates the loss between the predicted outputs and the true labels.\n",
    "loss.backward(): Performs backpropagation to calculate the gradients for each parameter in the network.\n",
    "optimizer.step(): Updates the network weights using the computed gradients.\n",
    "\n",
    "After each epoch, we print the average loss to monitor the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Convolutional layer 1: in_channels=1 (grayscale), out_channels=32, kernel_size=3x3\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        # Convolutional layer 2: in_channels=32, out_channels=64, kernel_size=3x3\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        # Max-pooling layer 2x2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Fully connected layer 1: 64*7*7 input to 128 output units\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        # Fully connected layer 2: 128 input to 10 output units (for 10 classes)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers followed by ReLU and max-pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Conv1 -> ReLU -> MaxPool\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Conv2 -> ReLU -> MaxPool\n",
    "        # Flatten the feature map from 2D to 1D (batch_size, channels * height * width)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        # Apply fully connected layers with ReLU activation\n",
    "        x = F.relu(self.fc1(x))  # Fully connected layer 1\n",
    "        x = self.fc2(x)          # Fully connected layer 2 (output layer)\n",
    "        return x\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "# Load dataset (MNIST)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate the network, loss function, and optimizer\n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # Loss calculation\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Concepts and Notes\n",
    "1. What is a CNN?\n",
    "\n",
    "    A Convolutional Neural Network (CNN) is a specialized kind of neural network for processing data with a grid-like topology, such as images.\n",
    "    CNNs are excellent for feature extraction using convolutions, which apply filters to identify edges, textures, and other spatial patterns in images.\n",
    "\n",
    "2. Layers in a CNN\n",
    "\n",
    "    Convolutional Layer (nn.Conv2d): This layer performs the convolution operation, sliding a kernel/filter over the input image to detect features. In this case:\n",
    "        conv1: 32 filters (feature maps) of size 3x3 applied to a grayscale image (1 channel).\n",
    "        conv2: 64 filters of size 3x3 applied to the 32 feature maps from conv1.\n",
    "    ReLU Activation Function: F.relu is used to introduce non-linearity into the network. It replaces all negative pixel values with 0, keeping the positive values unchanged.\n",
    "    Max-Pooling (nn.MaxPool2d): A pooling layer reduces the dimensionality of the feature maps. It downsamples the input by taking the maximum value from a 2x2 region, which helps reduce computational complexity and overfitting.\n",
    "    Fully Connected Layers (nn.Linear): These layers are used at the end of the CNN to make the final predictions. They take the flattened feature maps from the convolutional layers and output a prediction for each class.\n",
    "\n",
    "3. Forward Pass\n",
    "\n",
    "    The input passes through two convolutional layers (conv1 and conv2), with ReLU activations and max-pooling applied after each.\n",
    "    After the convolutional and pooling layers, the output is flattened (reshaped) into a 1D vector before passing through two fully connected layers to produce the final class scores.\n",
    "\n",
    "4. Loss Function\n",
    "\n",
    "    CrossEntropyLoss is commonly used for classification problems. It combines LogSoftmax and NLLLoss in one function to provide loss based on the predicted output and true label.\n",
    "\n",
    "5. Optimizer\n",
    "\n",
    "    Stochastic Gradient Descent (SGD): This optimizer updates the model's weights to minimize the loss function. You can adjust the learning rate and momentum to control the speed of convergence.\n",
    "\n",
    "6. Training Process\n",
    "\n",
    "    The training loop iterates through the dataset for a set number of epochs, computes the loss, and updates the network weights using backpropagation (loss.backward()) and the optimizer (optimizer.step()).\n",
    "    After each epoch, the average loss is printed to monitor the training progress.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Interview Concepts\n",
    "\n",
    "    Convolutional Filters:\n",
    "        Filters or kernels slide over the input to detect patterns.\n",
    "        Kernel size, stride, and padding influence the output size.\n",
    "        Learnable parameters: CNN filters are updated during training.\n",
    "\n",
    "    Pooling Layers:\n",
    "        Reduce the spatial size of the representation, downsampling by taking the maximum or average values.\n",
    "        Helps to make the model invariant to small translations in the input.\n",
    "\n",
    "    Activation Functions (ReLU):\n",
    "        Adds non-linearity to the network, which is essential for learning complex patterns.\n",
    "\n",
    "    Overfitting and Regularization:\n",
    "        CNNs are prone to overfitting due to their large number of parameters.\n",
    "        Techniques like dropout, data augmentation, or early stopping can mitigate overfitting.\n",
    "\n",
    "    Backpropagation and Gradient Descent:\n",
    "        During training, CNN weights are updated using gradient descent. The gradients of the loss with respect to each parameter are computed using backpropagation.\n",
    "\n",
    "    Applications of CNNs:\n",
    "        Image classification, object detection, face recognition, medical image analysis, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
