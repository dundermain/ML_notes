{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the Code\n",
    "\n",
    "    1. Class Definition (RNN):\n",
    "        __init__(): Initializes the RNN model with the number of input features (input_size), the number of hidden units (hidden_size), and the output size. The num_layers parameter allows stacking multiple RNN layers.\n",
    "        forward(): Defines how the input passes through the RNN. The hidden state is initialized to zeros. The RNN layer computes the outputs for each time step. Finally, the output of the last time step is passed to the fully connected layer (fc) to produce the final result.\n",
    "\n",
    "    2. h0: The hidden state initialized to zeros. This acts as the initial \"memory\" for the RNN. During training, the hidden state evolves as the RNN processes each time step.\n",
    "\n",
    "    3. RNN Layer (nn.RNN): This is the main recurrent part of the model. It processes the input sequence step-by-step, updating the hidden state at each time step.\n",
    "\n",
    "    4. Fully Connected Layer (nn.Linear): After processing the sequence, we often use a fully connected layer to map the RNN's output (at the last time step) to the desired output size.\n",
    "\n",
    "    5. Loss Function and Optimizer:\n",
    "        We use MSELoss here as an example (for a regression task). If you're working with a classification task, you might use CrossEntropyLoss instead.\n",
    "        The optimizer (Adam in this case) updates the model's weights based on the gradients computed during backpropagation.\n",
    "\n",
    "    6. Input Shape:\n",
    "        The input to the RNN is a tensor of shape (batch_size, sequence_length, input_size). In our case, we used batch_size=32, sequence_length=5, and input_size=10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the RNN model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Fully connected layer to output the result\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Get the outputs and the hidden state from the RNN\n",
    "        out, h_n = self.rnn(x, h0)\n",
    "        \n",
    "        # Pass the last time step's output to the fully connected layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 10   # Example: number of input features\n",
    "hidden_size = 20  # Number of units in the RNN cell\n",
    "output_size = 1   # Number of output classes (e.g. regression output or classification)\n",
    "num_layers = 1    # Number of RNN layers\n",
    "\n",
    "# Initialize the model, define the loss function and the optimizer\n",
    "model = RNN(input_size, hidden_size, output_size, num_layers)\n",
    "criterion = nn.MSELoss()  # Loss function (example: MSE for regression)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy input and target tensors\n",
    "inputs = torch.randn(32, 5, input_size)  # Batch size = 32, Sequence length = 5, Features = input_size\n",
    "targets = torch.randn(32, output_size)\n",
    "\n",
    "# Training step (single step example)\n",
    "outputs = model(inputs)\n",
    "loss = criterion(outputs, targets)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Concepts for RNNs\n",
    "\n",
    "    1. Sequential Data:\n",
    "        RNNs are designed to handle sequential data, where the order of the data points matters (e.g., time series, text, speech).\n",
    "        Unlike traditional feedforward neural networks, RNNs have \"memory\" that captures dependencies across time steps.\n",
    "\n",
    "    2. Hidden State:\n",
    "        At each time step, the RNN takes the current input and the hidden state from the previous step. The hidden state helps the network retain information about previous inputs in the sequence.\n",
    "\n",
    "    3. Backpropagation Through Time (BPTT):\n",
    "        In RNNs, backpropagation is done through the sequence of time steps. This is known as BPTT. The gradients are propagated back in time for each time step in the sequence, which can lead to issues like vanishing or exploding gradients for long sequences.\n",
    "\n",
    "    4. Types of RNNs:\n",
    "        Vanilla RNN (like the one in the code) processes sequences step-by-step.\n",
    "        LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) are advanced types of RNNs designed to handle long-range dependencies better and mitigate the vanishing gradient problem.\n",
    "\n",
    "    5. Applications:\n",
    "        RNNs are widely used in natural language processing (NLP), time series forecasting, speech recognition, and other tasks involving sequential data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interview Preparation Tips\n",
    "\n",
    "    Understand the difference between RNN, LSTM, and GRU: These are common interview questions. Be ready to explain how LSTMs and GRUs solve the vanishing gradient problem and when to use them over a basic RNN.\n",
    "\n",
    "    Backpropagation Through Time (BPTT): Be comfortable discussing how RNNs handle gradients over time and the challenges that come with it (vanishing and exploding gradients).\n",
    "\n",
    "    Application-based Questions: Be prepared to discuss how RNNs can be applied in real-world problems like text generation, time series analysis, or machine translation.\n",
    "\n",
    "    Optimization techniques: Interviewers may ask about tricks to improve training like gradient clipping or using LSTMs/GRUs for longer sequences.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
